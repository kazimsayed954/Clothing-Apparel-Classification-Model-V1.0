{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clothing_apparel_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV1hQgL3WLFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqebkbooWPCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPifqO1WSol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "73aaa958-e9f3-4564-b716-efaf9f23522a"
      },
      "source": [
        "print (\"Number of samples/observations in training data: \" + str(len(x_train)))\n",
        "print (\"Number of labels in training data: \" + str(len(y_train)))\n",
        "print (\"Dimensions of a single image in x_train:\" + str(x_train[0].shape))\n",
        "print(\"-------------------------------------------------------------\")\n",
        "print (\"Number of samples/observations in test data: \" + str(len(x_test)))\n",
        "print (\"Number of labels in test data: \" + str(len(y_test)))\n",
        "print (\"Dimensions of single image in x_test:\" + str(x_test[0].shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples/observations in training data: 60000\n",
            "Number of labels in training data: 60000\n",
            "Dimensions of a single image in x_train:(28, 28)\n",
            "-------------------------------------------------------------\n",
            "Number of samples/observations in test data: 10000\n",
            "Number of labels in test data: 10000\n",
            "Dimensions of single image in x_test:(28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrpEjN1JWYzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-caPgcGWcFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "b57c52a6-9067-49f6-a9ee-8f10125acc61"
      },
      "source": [
        "plt.subplot(231)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(232)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(233)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(234)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "plt.subplot(235)\n",
        "random_num = np.random.randint(0,len(x_train))\n",
        "plt.imshow(x_train[random_num], cmap=plt.get_cmap('gray'))\n",
        "\n",
        "\n",
        "# Visualize the images\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2de5BU1bX/v0vkIU8BYRh5RxF5KaYQSbQSiGJxNVWYB1bUMqRiBU28iVQsDWpZJmUSMRpzy8otDSFU0FgYU5JIikSDxB9EcoMBg/IKDxHC4AwzyBtFRPbvj2m3ay/nnOnp7unu3f39VFGzdq/uc3afdXpzzvestbc450AIISQ+Tit1BwghhOQGB3BCCIkUDuCEEBIpHMAJISRSOIATQkikcAAnhJBIyWsAF5FpIrJFRLaLyJxCdYqUFsa1cmFsKwvJNQ9cRDoA2ApgKoA6AP8EcJ1zblPhukeKDeNauTC2lcfpeXx2IoDtzrkdACAiTwOYDiDxZBCRsq0a6tWrl7cPHTqU83bOPPNMbx88eDCvPrUnzjlJcFVUXKuNlLgCbYwt41pW7HPO9bMv5jOADwSwW7XrAFxi3yQiswDMymM/ReGyyy7z9tKlS3PezpQpU7z9+9//Pq8+lYiKiisJaDW2jGvZsqulF/MZwLPCOTcPwDyA/6NXEoxrZcK4xkU+A/geAINVe1DmtSgZNWqUt9etWxf49uzJ/mtdcslHFzSRXoFXVFxJAGNbYeSThfJPACNEZLiIdALwFQBLCtMtUkIY18qFsa0wcr4Cd86dFJH/BvACgA4AFjjnNhasZ6QkMK6VC2NbeeScRpjTzspYU3vyySe93a1bt8D3gx/8wNsffPBB4PvqV78atHU2y80331zILhaUVrIV2kQ5x7XaYFwrlrXOuQn2RVZiEkJIpHAAJ4SQSOEATgghkdLueeDlSo8ePYJ2bW2tt/ft2xf45s6d6+3TTgv/z7PVlm+//ba3+/btm+gjhJB84RU4IYRECgdwQgiJlKqVUNLo2rVr0H733Xe9bdMu7Xs7d+7s7VOnTrVD7wghpBlegRNCSKRwACeEkEjhAE4IIZFStRr4GWecEbQ7derk7RMnTgS+jh07etumEVqdW39Wa+eExEbnzp0xdOhQ3/7Wt77l7QcffDB4b319fdH6FRP6mRgAjBgxIvG9euzYunVrVtvnFTghhEQKB3BCCImUqpVQGhsbg/bw4cO9vXFjOMOmlknsbIS2rdMMRQo2MRwhRWfIkCH4+c9/7tvDhg3ztl46EAC2bNni7ZMnTwY+LUEePnw48Onq5HfeeSfw6d/P+++/n9hP+xvs2bNn4nt1X4Dwt/3ee+8FvuPHj3u7X79wOUq99q2VXDt06OBt+50uuOACb9uUZF25vXv37sB31VVXoSV4BU4IIZHCAZwQQiKFAzghhERK1WrgaVjtWmtsVifTehcQpg1pnQxgWiGJi4aGBjz00EO+/fDDD3vbasL63F68eHHgGzRokLft7J36t2Zn79R6tdWLtc9q4FaT1tjf69GjR73dvXv3wLd+/Xpvv/7664Hvk5/8pLdPPz0cRvVspvv37w98+vtPmzYt8OnVvJ5//vmWv4CBV+CEEBIpHMAJISRSqlZCsbMI6tswW12pqy/tLZhNmdILIo8fPz7wsVqtMhg7dmzQ3rBhQ07b0fJBWsppqWa1PHLkCF566SXfHjhwoLe1vGCZNGlS0NaSgk5FBML0QJty16VLF2/b352WO+wi5Fpu0amAdn9AKAXZ9MM+ffp421ZG6kpuW22pxxKbNqnPFVuVqSXXVatWIRt4BU4IIZHCAZwQQiKFAzghhERK1WrgOg0ICHUzm+6nU5ZsypDVLvV2rrzyysD35z//ObfOkqJz//33B+3Ro0d7+7Of/Wzg06l2dpY+rd3aVDitbdvzSmu1n/70pwOffn7z4osvtvwFCoR+xrN582Zvv/HGG8H7tA7cv3//xO3Z34v+LvYY6HRAO3uoxqbq6VRfmxpo2zYmGq35Dx48OPDpMcHuX+veZ599duDTx6Z3796Bb8iQId5+8803E/ul4RU4IYRESqsDuIgsEJFGEdmgXusjIstEZFvmb++0bZDyg3GtXBjb6kHSbiEAQEQ+A+AogCecc2Mzr/0EwH7n3FwRmQOgt3Pue63uTCR9Z0XklltuCdrf+c53vL1t27bApxd7sMfLtnUKkb3tmzp1am6dbR8+iwqMq70Nt2meGp0e+tZbbwU+m1KmK+hs5V+PHj28bVPDmpqaWulx69hz7Prrr/f2okWL7HulUL9ZG9dHH33U21aK0JLKgAEDAp8+PnY2QJ3md8455wQ+nUbY0NAQ+HTK3YEDBwKfjp1dgMWi32tTFXXbpgrq88XOHKilMS3DAMCECRO8feTIkcCnv5OeHRUA9u3bt9Y5NwGGVq/AnXMrAew3L08HsDBjLwRwTWvbIeUF41q5MLbVQ64aeI1z7sOqlAYANQXqDyktjGvlwthWIHlnobjme7bEW2gRmQVgVr77IcWFca1c0mLLuMZFrgP4XhGpdc7Vi0gtgMakNzrn5gGYB5SXVqrTwoCPz5Km0ZqWLWu2eqjW3Gya0Lnnnuvt7du3Z9/Z4lG2cdXpZ2nPbdI07+9///tB+0tf+pK37dQKe/bsSWzbWSY1dqWnX/7yl97WaXhAeH58/vOfD3wXXXSRt19++eXAZ3XvLMkqtmlx1brvoUOHgs/pmQTHjRsX+PQKV7bsXadB/uc//wl8l156qbdtCbzWnfWqPkBYrm9/nzbFUffH7kPr9XV1dYFP69dW59bPYez+V6xY4e3zzz8/8OnnCnr6gTRylVCWAJiZsWcCeC7H7ZDygnGtXBjbCiSbNMJFAP4PwEgRqRORmwDMBTBVRLYBuCLTJhHBuFYujG310KqE4py7LsF1eYH7UlR01RMQzoSm0waB7KvpgPAW3qYelZOEUg5xTUvxsse1tXTXD/na174WtHW66MUXXxz4tCwzceLEwDdq1Kig/cQTT3jbShr6NtkuJvCNb3zD23rxACC8ZZ4/f37gmz59urdtmtpNN93kbS1PfDhDYHvFVp+zNl1Sz9Bo5UgtT1mZ4nOf+1yiT++vpiZ85qqPna2q3rlzp7etZGPTH/U5aOUOXZFtqy3197Wf0/KSXaRCf0c7U6KV2LKBlZiEEBIpHMAJISRSOIATQkikVO1shLbMPW3VE61x2dnU7HaOHTvmbZvSZrX1SkUfo7QVjNqy0oxeHeVHP/pR4Lvhhhu8rcu2gVC7vP322wOfXjnmt7/9beCzZd36mckPf/jDwPfaa69526Z/6TJvG3+djmjT2x555BFvf/nLX0YSd955p7ezncEuVzZt2uTtn/70p4FvyZIl3taLGAPhTH52FkN9XHWMgfBc0WX1QHhc7bMU/Zu0ZfZWk047V7Vebn/nemZRnfIJhGXwdooGPVOifSayZs0atBVegRNCSKRwACeEkEipWgkl11nr9C0Q8HEZQLdbm7mwUtHfM+24WnSa5d133x349Ax89tbzF7/4hbfvuOOOwKcXo7WzQV599dXeXr16deCzsdK32jbmP/7xj71tF/vV1Xx20V5dzafT6Sx2m1rS07foNg2v0Oj0QL0YsMVKRVp+uPDCCwPf2rVrvb1y5crAp6sfdTomAJx11lnetrKVnmHQ9tPKJDoGNu1XH8+//OUvgU9XX2oZCwhTV+3iH7pS187MuG7dOrQVXoETQkikcAAnhJBI4QBOCCGRUrUaeFpZs10pQ5fj2tJcW0artVObcmhncKtUtAZpNUDdvuKKKwLfyJEjvW1XYNGrJNnUMK052ikKzjvvPG+nzSRptdGlS5cGbb1PO8ukTpvTZe5AmGJo9V+tudpzTp8rVjvXK97oMvXWVp/Jl71793rbzhyoZwC0/dDH3R5nPSNfbW1t4NPnh31mpbfTq1evwKc1ePs712X2ADB06FBv2xkp9Tloy951zGfMmBH4/vSnP3nbTjmgj409x1lKTwghVQQHcEIIiRQO4IQQEilVq4Hb6SG1pnX48OHApzVImytqSSsPtznklUKXLl2CHO6f/OQn3k5bvUZPOwCEpcRWj9Sl1Da3V+vjjz/+eODTuqrVZnXu9/PPPx/4pk2blthXXVJuseXsWuOtr68PfDp/2ead6+lsbYm5PjZbtmzxttVpC41+ZmCfQ+icZlv2rrG/AR1LO32B/t3ZZwQ6R9v+5rQGbusQ7O9XnxP2O+nnXbfddlvg06sA6akCgFD31nUI9r12ymK7ClQ28AqcEEIihQM4IYREStVKKDalT9922ZQl3W7Lijxpvkqia9euGD9+vG/r1D1b5qyPX9rtrE3B1CvdWJ9ub9iwIfDdd999Le4bAJ5++mlvP/DAA4HPSmxantBpfLZvaSvA2DRCfX7YMvjJkyd72y7ArSUjW35eLGx5v5Y/7ApCZ599trdtmb1e9cau5KN/L/Y3qc+VtOks7HG1Eo5u674AYTqsPR/0dm1JvJZJdOolEKY12sWQ7cLJ2cArcEIIiRQO4IQQEikcwAkhJFKqVgNvamoK2jr1ya4srst2W9Ox06aMtRpbpXDw4EH84Q9/8O0JEyZ4267OoldgsamCWj+26Yda505LU9OrtANhufJdd92V6Pvd734X+Oz5oVMA08re7fmhzwerj9u0uaS+vf7664FPPzvQx6KYaap6GlggnArYnuc6drZ8XP/W7DMRrXvbcnmtgVtdXac7Wu3cno86VdGu5pQ21YJO7Ux7LmbPY73NZcuWBb5cppvmFTghhEQKB3BCCImUqpVQbKqTvhW1t8H6NsymJdkUJn27aG+J7MyFlcKpU6eC9KjZs2fntB2djmUXn9U+m0aob2ft7HM6lrZSUd+GW9nM3s6nnR96xRl7y64/p9PpLPYWXc+SN3bs2MCnv78+Tv/4xz8St58rSefzCy+8ELzv1ltv9batKNQpePb3olP17DHXMdfvA8LjZStstYRj0//sdvRn7XaSjrPFykJ6O1b60ZWn8+fPT9ymPceT5BVegRNCSKRwACeEkEhpdQAXkcEi8pKIbBKRjSJyW+b1PiKyTES2Zf72bm1bpHxgXCsTxrW6yEYDPwngdufcqyLSA8BaEVkG4GsAljvn5orIHABzAHyv/bpaWGz5a7armViN02qnGqt5a620DCi7uOrjZVeh0ZqgPY66BNtqyVqDtmmLemVzq81aPdbOVJfUNxtzvV1bVq3T32wKW9pMfHrVIT3bXUbjL2hck7RXu7KNnhVRp4oCoSb91ltvBT5dPm51Zr3yUVp6po25Tuu0KzSNGzcuaOtja6fX0LGzOrdO2bSpifqZjJ1xUK88/9hjjyGJbFMKWx21nHP1zrlXM/YRAJsBDAQwHcDCzNsWArgmqz2SsoBxrUwY1+qiTVkoIjIMwEUAVgOocc59WN3QAKAm4TOzAMzKvYukvWFcKxPGtfLJegAXke4AngUw2zl32KQXORFp8ZrfOTcPwLzMNtpeatRO2Ko4/X2sTJJG2q2O9dnFcMuBcoqrvmW1i2poqmVx6Hwodly//vWve3vOnDmBb8qUKd7WC1wA6YsTaxnLVlum/e50GqOVpsaMGZO4/7QZD63EmrZ4hpZVn3322cB37733Jn4uF7ISfkWkI5pPhqecc4szL+8VkdqMvxZAY0F7RtodxrUyYVyrh2yyUATArwBsds49olxLAMzM2DMBPFf47pH2gnGtTBjX6iIbreBSADcCWC8iHz5CvRvAXADPiMhNAHYBuLZ9ukjaCca1MmFcq4hWB3Dn3MsAJMF9eWG7UzxsqarGphjqtk0TsylM9rOampoWnxuVhEqNa7XTnnFNmyZCp+Pdc889iduwKXfDhg3ztn0upXVnq08nzcgIhL9B20+7D/37tamCOkXTluDr5zB26gC7ClV7wkpMQgiJFA7ghBASKVU7G6GtytOpP1YG0beOVnqxkor+rJ250FaoERITuSw4YLEL9+aykC/5CF6BE0JIpHAAJ4SQSOEATgghkUINPIOe7cyW7aaV1FpdUG/HLjJbTmmEhJD44RU4IYRECgdwQgiJlKqVUOwCs01NTd5OSxW0E//bNEItsRw7dizw9e/fP7fOEkJIC/AKnBBCIoUDOCGERAoHcEIIiZSq1cDtLIKTJk3y9tq1awOfLrO3Gni/fv2C9sCBA71tZ1A7evRobp0lhJAW4BU4IYRECgdwQgiJFCnEDGNZ76yMFjW2XHnlld4+77zzEn19+/YNfKtXrw7ar732mrdXrFgR+Hbu3JlvNwuGcy55RYs2Us5xrTYY14plrXNugn2RV+CEEBIpHMAJISRSOIATQkikFFsDb0LzithnASjeyp/pVGNfhjrn+rX+tuxgXFuFcS0c1dqXFmNb1AHc71RkTUuCfClgXwpHOfWffSkc5dR/9iWEEgohhEQKB3BCCImUUg3g80q035ZgXwpHOfWffSkc5dR/9kVREg2cEEJI/lBCIYSQSOEATgghkVLUAVxEponIFhHZLiJzirnvzP4XiEijiGxQr/URkWUisi3zt3cR+jFYRF4SkU0islFEbitVXwoB4xr0pWJiy7gGfSnLuBZtABeRDgD+F8B/ARgN4DoRGV2s/Wf4NYBp5rU5AJY750YAWJ5ptzcnAdzunBsNYBKAWzPHohR9yQvG9WNURGwZ149RnnF1zhXlH4BPAXhBte8CcFex9q/2OwzABtXeAqA2Y9cC2FKCPj0HYGo59IVxZWwZ13jiWkwJZSCA3apdl3mt1NQ45+ozdgOAmmLuXESGAbgIwOpS9yVHGNcEIo8t45pAOcWVDzEVrvm/0aLlVYpIdwDPApjtnDtcyr5UMqU4loxt+8O4FncA3wNgsGoPyrxWavaKSC0AZP42FmOnItIRzSfCU865xaXsS54wroYKiS3jaijHuBZzAP8ngBEiMlxEOgH4CoAlRdx/EksAzMzYM9GsbbUrIiIAfgVgs3PukVL2pQAwrooKii3jqijbuBZZ+L8KwFYAbwC4pwQPHhYBqAfwPpo1vZsA9EXz0+NtAF4E0KcI/bgMzbdarwNYl/l3VSn6wrgytoxrvHFlKT0hhEQKH2ISQkikcAAnhJBIyWsAL3WpLWkfGFdC4iBnDTxTarsVzdVIdWh+an2dc25TymcouJcJzjlp6fVqjetpp4XXMj169Aja3bp18/a7774b+A4cONB+HWsjSXEllcnpeXx2IoDtzrkdACAiTwOYDiDxh06ioCzj2pzF1Ux7PHjv3r170P7MZz4TtCdOnOjt9evXB75nnnkmcbv6PwbbbyYQkHzJR0LJqtRWRGaJyBoRWZPHvkjxYFwJiYR8rsCzwjk3D5mlhyrhVps0w7gSUnryGcDLtdSW5EdZxNVq0qdOnfJ2TU04X9B9993n7ffeey/xc+edd17gGzlypLe1xg0AW7duDdqdOnXy9tVXXx347r//fm+PGTMm8J08edLbHTp0CHwffPABCMmHfCSUci21JfnBuBISCTlfgTvnTorIfwN4AUAHAAuccxsL1jNSEhhXQuKhqKX01ErLh0KmmxUqrtlmmixevDhoT5482dt1dXWB7+WXX/b2hAkTAp+WV/bu3Rv4OnbsGLS7dOmS2LdBgwZ5e8GCBYHvwQcfTNymllcK9TtkGmF1wUpMQgiJFA7ghBASKRzACSEkUto9D5yQbNFpdlofttTW1gbtLVu2ePvQoUOJn9u5c2fQPv30j05/rXEDwBlnnJG4Hfved955x9sXXHBB4ufef//9oK01f0JygVfghBASKRzACSEkUiihkLLBVl9qrr32Wm/37Nkz8DU2frSOrK6YBIBx48Z5+8SJE4EvTcKwfdHvtRWVuvrz3HPPDXy6r4cPB4uYZy0ZEZIEr8AJISRSOIATQkikcAAnhJBIoQbejvztb38L2o8//ri3n3rqqYLsQ+uoVtPVs93FsHiA1qhtqt7s2bO9bWcczFVL1sfL7s8eS62J25J43R+7MITW7ufPn5+4TUJygWcQIYRECgdwQgiJFEooBWbSpEnePn78eOC74YYbvH3OOecEvoceesjbdtHcNCp1UYB77703aPfr18/bOm0QCOUhuzBD586dvW0rIbX0kraABJA+U6L22X3ceOON3rYSik1rJKSt8AqcEEIihQM4IYRECgdwQgiJFGrgWaD1UauNjh07Nmjr1WI2bgxXItP66Le//e3AN2PGDG/bWfPOPPNMb7/11luBT8+EZxfifeCBBxArtlxeHzsbA53WZ8vctT5tt6lnI7Tph2ll9k1NTYnbsc89unbt6u0BAwYEvoaGhsR9EJINvAInhJBI4QBOCCGRQgklC/Rtub19/+53vxu09+zZ4+0jR44EvpqaGm/r224glGLOP//8wKfTzdJ8vXr1avkLRMiTTz4ZtPXCxTZ1UktcNh1QV0na9D+9aIP12RQ/nfY5bNiwwLdmzRrbfY9Oa/zCF74Q+B577LHEzxGSDbwCJ4SQSOEATgghkcIBnBBCIoUaeBbY8mjNxRdfnPherXkDoR6qS7wBYP/+/d62Gq9OcdMaOxBqt8eOHUvsZ2y88sorQbtPnz7etivbaL3axkqnYNpZBPXn7DG3aYT6mcWiRYsC3zXXXOPtf/3rX4FPx2fq1KmBjxo4yRdegRNCSKS0OoCLyAIRaRSRDeq1PiKyTES2Zf72bt9ukkLDuBISP9lIKL8G8HMAT6jX5gBY7pybKyJzMu3vFb575cc3v/nNoG2lkIMHD3r7wIEDga9374/Gw927dwc+LQukVQXaNEZd6bdly5bUvht+jTKO66xZs4L2vn37vG1TBfVCxrbaUssmdgZILUfZGQ6HDh0atPVskffcc0/g0+miWuoBQlnLniuE5EurV+DOuZUA9puXpwNYmLEXArgGJCoYV0LiJ9eHmDXOufqM3QCgJumNIjILwKwkPykrGFdCIiLvLBTnnBORxAUXnXPzAMwDgLT3kfKCcSWk/Ml1AN8rIrXOuXoRqQXQ2OonKoQ77rgjaFvtVGvZWvMGQr3apqlpfTRtJjybKqi3qVPmcqRs4mpX5Kmvr/e2nYZAPzOw0xdofdw+d0jT1W0pvdW9NSNHjvT2X//618D39ttve9ueDzpe+tkJIdmSaxrhEgAzM/ZMAM8VpjukxDCuhERENmmEiwD8H4CRIlInIjcBmAtgqohsA3BFpk0ignElJH5alVCcc9cluC4vcF8KTmsL1WbLiy++6G2b4mcXIO7Ro4e3a2trA5+u9rMySVqqoJ5Rzy4YoOWEiRMntvwFWqAc4zpz5kxvW5lEHzubjqerHe2sgnomSbuAgpaf7EIQq1atyrbbwSIbq1evDnwXXniht/XiGwAwcOBAb1NCIbnASkxCCIkUDuCEEBIpHMAJISRSKno2Qqujam3ZatlnnXWWt597Lky+0FrlG2+8Efis5jp48OAW99dSfzRp+rjWZ+2MevpzerbDGLniiiu8bdMB9TGwx0c/67DHR7ft8wr9bEHr4S21s+Xf//530NYzEO7atSvw6UWO7QLYhGQDr8AJISRSOIATQkiklI2EYtO49G2yvWXWsoWVKXTbVtNppkyZErSXLl3qbXsbvH79em/rNEEA6Nu3b9DW0kyXLl0S+5bW77akO+oFDGxf9Ox7VvopR/QMgLbiVB/LNIlJV14CoYRi5RWdjmgXghg+fHi23Q7YsWNHYt/s/j/xiU94e/ny5Tntj1Q3vAInhJBI4QBOCCGRwgGcEEIipaQauNYy7aKyhcCW0o8aNcrbd999d+DTi9Fu37498I0YMcLbVgO3/dapgnbxXY1NP9R9tc8DtHZq9V+t89sS88suu8zbMWjgTU1N3h42bFjgO3r0qLetlqxJK6W3zyR0quChQ4cCn00zzfZ5gvXpfdo0UrvoNSFthVfghBASKRzACSEkUjiAE0JIpJRUA9d6pdV9tT44ZMiQwKe17HPPPTfw6Slce/XqFfi0HmnzfnXptl2RXOcWW23U6uzab7XatO+r29an96G1YCDMGbffafz48d5euHAhyp1169Z5e/LkyYFPP2uwWrI+rrZcXh8fq4Hr5wl2m7b9qU99yttpGnhdXV3Q1rGzzy+ogZN84RU4IYRECgdwQgiJlJJKKNOmTfP2zTffHPjSVlnRUoGVDfSqJ/v37w98+nZ27NixgU+n4NmVU7TPpurZ0u2k/QHp5fJaerG+tKkD9D7scbIpj+WOlh+sVKUlFHvMdSqlnT4hbUqGtBkgLV/84he9/Zvf/CbxfXYKAD3jod1HARahJlUOr8AJISRSOIATQkikcAAnhJBIKaoG3qVLlyDt78477/S2Tb96++23vZ2WOmdXTtFTqvbs2TPw6bQ+WwKv9Ui7Ta17Ww06bcpaq0lnO2WsTSPUKW22b/oZgN2fPW7lTprWr59L2GcL+nNp08namOuUw7TnLECYRpg2vbFdSUifx7ZvNq2RkLbCK3BCCIkUDuCEEBIpRZVQBg8ejIcffti39S2sragcM2aMt21an8ZKGGkLB2v5wd6G677YW9vjx497O+32GQjljzRf2qK5NoVOywm23/pW396+X3755Yn7KEfS0h71cbfHNdcKV33uWHnFtvfs2ePtfv36Bb7GxsbEfjc0NHhbL3jd0j4IaSu8AieEkEhpdQAXkcEi8pKIbBKRjSJyW+b1PiKyTES2Zf72bv/ukkLBuBISP9lcgZ8EcLtzbjSASQBuFZHRAOYAWO6cGwFgeaZN4oFxJSRyWtXAnXP1AOoz9hER2QxgIIDpACZn3rYQwP8D8L20bR05cgQrVqzwbV2efPDgweC9ugTZ6sU6HStN807TSi02bUyjdee0NDX7Xts3raXr9LK0/QHpKYdaL9fHDAh144EDB3q7sbERJ06cKFhcC4WeddLGQ+vFaVMNWPR70zRnG1e7TX0s084VS58+fbxtz5XevXlzQ/KjTQ8xRWQYgIsArAZQkxncAaABQItzY4rILACzgI/nZZPyIN+4EkJKQ9YPMUWkO4BnAcx2zgWLPbrmy5UWL4Occ/OccxOccxPSMi9IaShEXIvQTUJIC2R1BS4iHdH8I3/KObc48/JeEal1ztWLSC2A5FyqDBABp/MAAAVjSURBVA0NDXjggQd8e8OGDd6+5ZZbgvcOGDDA22kz8KXNWmclDH0bbGcVTJt4X1c02s+9+uqrQbu+vt7bK1euDHwTJ0709owZMwLfgQMHvG3TGPX3sDKJ9mmJBghlAJ2i96F8U6i4Foqzzz7b2zY9NG3WRd1OW/TZHp+0BR26desWtHfu3OltHavW0OenTWPkBQ3Jl2yyUATArwBsds49olxLAMzM2DMBPFf47pH2gnElJH6yuQK/FMCNANaLyIdrXt0NYC6AZ0TkJgC7AFzbPl0k7QTjSkjkZJOF8jKApNnu4yr1Ix7GlZD4KemKPH/84x9btAFgwoSPno1de214EThu3DhvDxo0KPD179/f2zZtTGugu3btCnw6NWzZsmWB72c/+1nLX6CNPP300962GrjW/PWzASDU3UeMGBH4Vq1a5e2mpqbApzVWnbZonxuUC3pGSPvcQ+vVtv9pqaR6O1Yf12l91mc18LSZHdPK/P/+9797+/rrrw98aas5EZINLKUnhJBI4QBOCCGRUlIJJe3Wc82aNS3a5UauVZO1tbXt1qdY0ccy7bimLehgpRDdtpWQadKL3cebb76Z+F6dHmjlnR07dnjbpoe+8soridskJBt4BU4IIZHCAZwQQiKFAzghhERKSTXwtFnkYiFN8yZtQ+vVaTMOpq2KZH1ay7ZppVoDT1sFCQhX1rFYvTxpO1YD379/f+LnCMkGXoETQkikcAAnhJBIKamEQohGL15tZQktcaRVadoZ/6ykotFpha3Jedu2bUv0pUkojz76qLdttXFdXV3qPglpDV6BE0JIpHAAJ4SQSOEATgghkUINnJQNffv29Xa/fv0Cn14cOG3BYauBa6xWrVfrsSmGegUjANizZ0/idvVnreaufdu3b0/cBiG5wCtwQgiJFA7ghBASKZRQSNmgF8645JJLAt+hQ4e8bRcDTquG1ZKKXZBaV2JaCWXkyJFBW88qaNGSTlvkHf1eu39CsoFX4IQQEikcwAkhJFI4gBNCSKRIMWcEFJEmALsAnAVgX9F2nE419mWoc65f62/LDsa1VaKMKyl/ijqA+52KrHHOTWj9ne0P+1I4yqn/7AupBiihEEJIpHAAJ4SQSCnVAD6vRPttCfalcJRT/9kXUvGURAMnhBCSP5RQCCEkUjiAE0JIpBR1ABeRaSKyRUS2i8icYu47s/8FItIoIhvUa31EZJmIbMv87V2EfgwWkZdEZJOIbBSR20rVl0LAuAZ9qajYkvKmaAO4iHQA8L8A/gvAaADXicjoYu0/w68BTDOvzQGw3Dk3AsDyTLu9OQngdufcaACTANyaORal6EteMK4fo2JiS8qfYl6BTwSw3Tm3wzl3AsDTAKYXcf9wzq0EsN+8PB3Awoy9EMA1RehHvXPu1Yx9BMBmAANL0ZcCwLiGfamk2JIyp5gD+EAAu1W7LvNaqalxztVn7AYANcXcuYgMA3ARgNWl7kuOMK4JVEBsSZnDh5gK15xTWbS8ShHpDuBZALOdc4dL2ZdKphTHkrElxaCYA/geAINVe1DmtVKzV0RqASDzt7EYOxWRjmj+gT/lnFtcyr7kCeNqqKDYkjKnmAP4PwGMEJHhItIJwFcALCni/pNYAmBmxp4J4Ln23qE0L9PyKwCbnXOPlLIvBYBxVVRYbEmZU+zpZK8C8D8AOgBY4Jz7UdF23rz/RQAmo3l6z70A7gPwBwDPABiC5ilRr3XO2Qdihe7HZQD+BmA9gA/XA7sbzVppUftSCBjXoC8VFVtS3rCUnhBCIoUPMQkhJFI4gBNCSKRwACeEkEjhAE4IIZHCAZwQQiKFAzghhEQKB3BCCImU/w+zUuSjTRFV8gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EHACsGeWeKc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "2ab1d562-c7fa-4bad-ccc1-87e29560a901"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import backend as K\n",
        "\n",
        "# Setting Training Parameters like batch_size, epochs\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "# Storing the number of rows and columns\n",
        "img_rows = x_train[0].shape[0]\n",
        "img_cols = x_train[1].shape[0]\n",
        "\n",
        "''' Getting the data in the right 'shape' as required by Keras i.e. adding a 4th \n",
        "dimension to our data thereby changing the original image shape of (60000,28,28) \n",
        "to (60000,28,28,1)'''\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "\n",
        "# Storing the shape of a single image \n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Changing image type to float32 data type\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalizing the data by changing the image pixel range from (0 to 255) to (0 to 1)\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Performing one hot encoding\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Calculate the number of classes and number of pixels \n",
        "num_classes = y_test.shape[1]\n",
        "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
        "\n",
        "# Create CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = keras.optimizers.Adadelta(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,200,778\n",
            "Trainable params: 1,200,330\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwCwkkvrWlwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b3cfc23-0f47-4b5f-e996-d919eb49f707"
      },
      "source": [
        "model_fitting = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 23s 386us/step - loss: 0.4525 - accuracy: 0.8456 - val_loss: 1.1607 - val_accuracy: 0.6548\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.2854 - accuracy: 0.8989 - val_loss: 0.2505 - val_accuracy: 0.9082\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.2327 - accuracy: 0.9170 - val_loss: 0.2365 - val_accuracy: 0.9161\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.2085 - accuracy: 0.9257 - val_loss: 0.2284 - val_accuracy: 0.9164\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.1872 - accuracy: 0.9319 - val_loss: 0.2195 - val_accuracy: 0.9234\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.1693 - accuracy: 0.9388 - val_loss: 0.2176 - val_accuracy: 0.9230\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.1540 - accuracy: 0.9449 - val_loss: 0.2232 - val_accuracy: 0.9249\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 17s 279us/step - loss: 0.1425 - accuracy: 0.9485 - val_loss: 0.2450 - val_accuracy: 0.9210\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.1293 - accuracy: 0.9538 - val_loss: 0.2078 - val_accuracy: 0.9280\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.1226 - accuracy: 0.9557 - val_loss: 0.2216 - val_accuracy: 0.9248\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.1155 - accuracy: 0.9586 - val_loss: 0.2579 - val_accuracy: 0.9275\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.1048 - accuracy: 0.9620 - val_loss: 0.2556 - val_accuracy: 0.9254\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0998 - accuracy: 0.9639 - val_loss: 0.2363 - val_accuracy: 0.9251\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0952 - accuracy: 0.9656 - val_loss: 0.3440 - val_accuracy: 0.9153\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0921 - accuracy: 0.9676 - val_loss: 0.2616 - val_accuracy: 0.9276\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0871 - accuracy: 0.9682 - val_loss: 0.2451 - val_accuracy: 0.9231\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 16s 275us/step - loss: 0.0812 - accuracy: 0.9708 - val_loss: 0.2548 - val_accuracy: 0.9306\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0796 - accuracy: 0.9714 - val_loss: 0.3333 - val_accuracy: 0.9223\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0750 - accuracy: 0.9730 - val_loss: 0.3046 - val_accuracy: 0.9284\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0706 - accuracy: 0.9738 - val_loss: 0.2833 - val_accuracy: 0.9278\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0708 - accuracy: 0.9751 - val_loss: 0.2885 - val_accuracy: 0.9239\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0646 - accuracy: 0.9765 - val_loss: 0.2811 - val_accuracy: 0.9305\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0633 - accuracy: 0.9768 - val_loss: 0.2704 - val_accuracy: 0.9316\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0635 - accuracy: 0.9775 - val_loss: 0.3324 - val_accuracy: 0.9227\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0601 - accuracy: 0.9784 - val_loss: 0.3160 - val_accuracy: 0.9296\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0586 - accuracy: 0.9786 - val_loss: 0.2918 - val_accuracy: 0.9307\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0584 - accuracy: 0.9783 - val_loss: 0.3100 - val_accuracy: 0.9315\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0553 - accuracy: 0.9799 - val_loss: 0.3227 - val_accuracy: 0.9344\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0551 - accuracy: 0.9807 - val_loss: 0.3006 - val_accuracy: 0.9286\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0532 - accuracy: 0.9807 - val_loss: 0.2717 - val_accuracy: 0.9269\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0525 - accuracy: 0.9814 - val_loss: 0.2935 - val_accuracy: 0.9298\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0493 - accuracy: 0.9820 - val_loss: 0.3676 - val_accuracy: 0.9308\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0474 - accuracy: 0.9835 - val_loss: 0.4342 - val_accuracy: 0.9304\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0488 - accuracy: 0.9830 - val_loss: 0.2891 - val_accuracy: 0.9305\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0470 - accuracy: 0.9832 - val_loss: 0.3154 - val_accuracy: 0.9332\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0453 - accuracy: 0.9841 - val_loss: 0.2621 - val_accuracy: 0.9276\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0452 - accuracy: 0.9842 - val_loss: 0.3384 - val_accuracy: 0.9303\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 17s 282us/step - loss: 0.0419 - accuracy: 0.9855 - val_loss: 0.2930 - val_accuracy: 0.9315\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0415 - accuracy: 0.9845 - val_loss: 0.2671 - val_accuracy: 0.9306\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0427 - accuracy: 0.9847 - val_loss: 0.2698 - val_accuracy: 0.9307\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0398 - accuracy: 0.9859 - val_loss: 0.3106 - val_accuracy: 0.9319\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0394 - accuracy: 0.9858 - val_loss: 0.3592 - val_accuracy: 0.9313\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0410 - accuracy: 0.9855 - val_loss: 0.2450 - val_accuracy: 0.9294\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0391 - accuracy: 0.9856 - val_loss: 0.4688 - val_accuracy: 0.9317\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0374 - accuracy: 0.9866 - val_loss: 0.3451 - val_accuracy: 0.9308\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0393 - accuracy: 0.9869 - val_loss: 0.3087 - val_accuracy: 0.9318\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0365 - accuracy: 0.9873 - val_loss: 0.3553 - val_accuracy: 0.9326\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0384 - accuracy: 0.9863 - val_loss: 0.3985 - val_accuracy: 0.9307\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0386 - accuracy: 0.9867 - val_loss: 0.4380 - val_accuracy: 0.9335\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0342 - accuracy: 0.9879 - val_loss: 0.2988 - val_accuracy: 0.9294\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0374 - accuracy: 0.9874 - val_loss: 0.4085 - val_accuracy: 0.9306\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0346 - accuracy: 0.9877 - val_loss: 0.4110 - val_accuracy: 0.9350\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0355 - accuracy: 0.9874 - val_loss: 0.3042 - val_accuracy: 0.9303\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0325 - accuracy: 0.9890 - val_loss: 0.3492 - val_accuracy: 0.9337\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 17s 279us/step - loss: 0.0342 - accuracy: 0.9884 - val_loss: 0.3198 - val_accuracy: 0.9339\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0354 - accuracy: 0.9880 - val_loss: 0.2899 - val_accuracy: 0.9280\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0346 - accuracy: 0.9877 - val_loss: 0.4524 - val_accuracy: 0.9322\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0299 - accuracy: 0.9897 - val_loss: 0.3710 - val_accuracy: 0.9326\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0327 - accuracy: 0.9888 - val_loss: 0.4480 - val_accuracy: 0.9336\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 17s 279us/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.3741 - val_accuracy: 0.9321\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0332 - accuracy: 0.9887 - val_loss: 0.5352 - val_accuracy: 0.9326\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0314 - accuracy: 0.9890 - val_loss: 0.3215 - val_accuracy: 0.9306\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 0.2572 - val_accuracy: 0.9232\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0318 - accuracy: 0.9891 - val_loss: 0.4779 - val_accuracy: 0.9314\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0317 - accuracy: 0.9887 - val_loss: 0.4693 - val_accuracy: 0.9318\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0316 - accuracy: 0.9893 - val_loss: 0.2936 - val_accuracy: 0.9237\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0294 - accuracy: 0.9899 - val_loss: 0.3385 - val_accuracy: 0.9316\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.3562 - val_accuracy: 0.9307\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0297 - accuracy: 0.9897 - val_loss: 0.4239 - val_accuracy: 0.9329\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0312 - accuracy: 0.9892 - val_loss: 0.5096 - val_accuracy: 0.9321\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0299 - accuracy: 0.9897 - val_loss: 0.5079 - val_accuracy: 0.9305\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0297 - accuracy: 0.9899 - val_loss: 0.3027 - val_accuracy: 0.9246\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0286 - accuracy: 0.9903 - val_loss: 0.5076 - val_accuracy: 0.9327\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0270 - accuracy: 0.9904 - val_loss: 0.4121 - val_accuracy: 0.9345\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0247 - accuracy: 0.9913 - val_loss: 0.4546 - val_accuracy: 0.9323\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0277 - accuracy: 0.9901 - val_loss: 0.4489 - val_accuracy: 0.9316\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0285 - accuracy: 0.9905 - val_loss: 0.5256 - val_accuracy: 0.9342\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.2779 - val_accuracy: 0.9218\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0265 - accuracy: 0.9908 - val_loss: 0.3870 - val_accuracy: 0.9327\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0250 - accuracy: 0.9915 - val_loss: 0.4280 - val_accuracy: 0.9336\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0250 - accuracy: 0.9915 - val_loss: 0.3563 - val_accuracy: 0.9328\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0269 - accuracy: 0.9908 - val_loss: 0.3691 - val_accuracy: 0.9265\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0262 - accuracy: 0.9909 - val_loss: 0.5299 - val_accuracy: 0.9324\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0256 - accuracy: 0.9909 - val_loss: 0.3376 - val_accuracy: 0.9326\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0266 - accuracy: 0.9909 - val_loss: 0.5203 - val_accuracy: 0.9297\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0253 - accuracy: 0.9916 - val_loss: 0.4933 - val_accuracy: 0.9339\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0242 - accuracy: 0.9914 - val_loss: 0.4542 - val_accuracy: 0.9326\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.5104 - val_accuracy: 0.9338\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 0.3881 - val_accuracy: 0.9322\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0256 - accuracy: 0.9913 - val_loss: 0.3515 - val_accuracy: 0.9347\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0244 - accuracy: 0.9915 - val_loss: 0.4353 - val_accuracy: 0.9340\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0241 - accuracy: 0.9917 - val_loss: 0.2720 - val_accuracy: 0.9294\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0230 - accuracy: 0.9924 - val_loss: 0.3755 - val_accuracy: 0.9313\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0231 - accuracy: 0.9919 - val_loss: 0.4139 - val_accuracy: 0.9328\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 0.5214 - val_accuracy: 0.9338\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 17s 276us/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.4233 - val_accuracy: 0.9339\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0251 - accuracy: 0.9915 - val_loss: 0.4622 - val_accuracy: 0.9334\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 17s 277us/step - loss: 0.0232 - accuracy: 0.9922 - val_loss: 0.4336 - val_accuracy: 0.9297\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 17s 278us/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.3869 - val_accuracy: 0.9358\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 17s 279us/step - loss: 0.0233 - accuracy: 0.9919 - val_loss: 0.5248 - val_accuracy: 0.9338\n",
            "Test loss: 0.52477780782021\n",
            "Test accuracy: 0.9337999820709229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqs0-pvodCMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3b05ffd5-f6c7-4373-b0ed-70f75e1aa1c3"
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.19-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.19-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.19-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNCI7gwqdecZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "232929a5-8ed1-4b35-c76a-ee7352a4e9de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye2nMp47doCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Agle84d2pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('clothing_classification_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Geq8xeEcczxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "cb9dc8f2-80e3-4b3c-eca6-d0038e9b9951"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Function to load and prepare the image in right shape\n",
        "def load_image(filename):\n",
        "\t# Load the image\n",
        "\timg = load_img(filename, grayscale=True, target_size=(28, 28))\n",
        "\t# Convert the image to array\n",
        "\timg = img_to_array(img)\n",
        "\t# Reshape the image into a sample of 1 channel\n",
        "\timg = img.reshape(1, 28, 28, 1)\n",
        "\t# Prepare it as pixel data\n",
        "\timg = img.astype('float32')\n",
        "\timg = img / 255.0\n",
        "\treturn img\n",
        "\n",
        "# Load an image and predict the apparel class\n",
        "img = load_image('sandal.jpg')\n",
        "# Load the saved model\n",
        "model = load_model('clothing_classification_model.h5')\n",
        "# Predict the apparel class\n",
        "class_prediction = model.predict_classes(img)\n",
        "print(class_prediction[0])\n",
        "\n",
        "#Map apparel category with the numerical class\n",
        "if class_prediction[0] == 0:\n",
        "  product = \"T-shirt/top\"\n",
        "elif class_prediction[0] == 1:\n",
        "  product = \"Trouser\"\n",
        "elif class_prediction[0] == 2:\n",
        "  product = \"Pullover\"\n",
        "elif class_prediction[0] == 3:\n",
        "  product = \"Dress\"\n",
        "elif class_prediction[0] == 4:\n",
        "  product = \"Coat\"\n",
        "elif class_prediction[0] == 5:\n",
        "  product = \"Sandal\"\n",
        "elif class_prediction[0] == 6:\n",
        "  product = \"Shirt\"\n",
        "elif class_prediction[0] == 7:\n",
        "  product = \"Sneaker\"\n",
        "elif class_prediction[0] == 8:\n",
        "  product = \"Bag\"\n",
        "else:\n",
        "  product = \"Ankle boot\"\n",
        "\n",
        "print(product)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-06120f6297f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load an image and predict the apparel class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sandal.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clothing_classification_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-06120f6297f7>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Convert the image to array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sandal.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVNQ3IvJDYEE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "9863111d-7f09-42cd-e1f1-91010adfd1cd"
      },
      "source": [
        "from tensorflow.contrib import lite\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-541ce0a8679e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}